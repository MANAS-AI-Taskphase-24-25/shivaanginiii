# -*- coding: utf-8 -*-
"""clusteringtask3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZHXlOuESoNiZAe99IxjNNNho5VLCyqqt
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import gdown
import pandas as pd

file_id = '1lwmokQo-3nw9SyrwxeljcAQlWvnEQ4OT'
output_file = 'Clustering_Data.csv'

# URL for the file
download_url = f'https://drive.google.com/uc?id={file_id}'

gdown.download(download_url, output_file, quiet=False)
clusteringset = pd.read_csv(output_file, sep=',')
clusteringset = pd.read_csv(output_file, sep=',')

clusteringset = clusteringset.drop(['Customer_Segment'],axis=1)
clusteringset

def mean_normalization(data):
    mean = np.mean(data, axis=0)
    std = np.std(data, axis=0)
    normalized_data = (data - mean) / std
    return normalized_data

def initialize_centroids(clusteringset, k):
    indices = np.random.choice(clusteringset.shape[0], k, replace=False)
    return clusteringset[indices]

def k_means(clusteringset, k, max_iters=100):
    # Convert clusteringset to a NumPy array for NumPy-style indexing
    clusteringset_np = clusteringset.to_numpy()
    centroids = initialize_centroids(clusteringset_np, k)

    for _ in range(max_iters):
        distances = np.linalg.norm(clusteringset_np[:, np.newaxis] - centroids, axis=2)
        labels = np.argmin(distances, axis=1)
        new_centroids = np.array([
            clusteringset_np[labels == i].mean(axis=0)
            for i in range(k)
        ])
        centroids = new_centroids
    return centroids, labels

features = clusteringset.select_dtypes(include=[np.number])
def costfunction(clusteringset, k):
    # Convert to NumPy array here as well to maintain consistency
    clusteringset_np = clusteringset.to_numpy()
    centroids, labels = k_means(clusteringset, k)
    total_distance = np.sum([
        np.sum(np.linalg.norm(clusteringset_np[np.where(labels == i)] - centroids[i], axis=1) ** 2)
        for i in range(k)
    ])
    return total_distance / len(clusteringset_np)

# Elbow method to find the optimal number of clusters
k_values = range(1, 10)
inertias = [costfunction(features, k) for k in k_values]

plt.plot(k_values, inertias, marker='o', label="Cost Function")
plt.title("Elbow Method for Optimal Clusters")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Cost Function")
plt.legend()
plt.show()

optimal_k = 3
centroids, labels = k_means(features, optimal_k, max_iters=10)

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
reduced_features = pca.fit_transform(features)
reduced_centroids = pca.transform(centroids)

for i in range(optimal_k):
    plt.scatter(reduced_features[np.where(labels == i), 0], reduced_features[np.where(labels == i), 1], label=f"Cluster {i+1}")
plt.scatter(reduced_centroids[:, 0], reduced_centroids[:, 1], color='black', marker='x', s=100, label="Centroids")
plt.title("Clusters Visualized with PCA")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend()
plt.show()