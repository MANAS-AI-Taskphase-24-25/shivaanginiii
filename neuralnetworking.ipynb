{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "s0_GjH4BvvV3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = (64, 64)\n",
        "LEARNING_RATE = 0.01\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "L2_LAMBDA = 0.01\n",
        "DROPOUT_RATE = 0.5"
      ],
      "metadata": {
        "id": "-uMxGY3bHMKZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import zipfile\n",
        "\n",
        "file_id = '1EF25ppO6mM_0eSlWzDrxszLt0Mmyr-wm'\n",
        "output_file = 'Train.zip'\n",
        "\n",
        "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "gdown.download(download_url, output_file, quiet=False)\n",
        "\n",
        "with zipfile.ZipFile(output_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"extracted_files/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLTlU9zRv69E",
        "outputId": "249b9544-3751-42e5-f5fa-f2d0c8d04dd5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1EF25ppO6mM_0eSlWzDrxszLt0Mmyr-wm\n",
            "From (redirected): https://drive.google.com/uc?id=1EF25ppO6mM_0eSlWzDrxszLt0Mmyr-wm&confirm=t&uuid=dabaf1d3-0461-44e6-94f4-ed133196f478\n",
            "To: /content/Train.zip\n",
            "100%|██████████| 2.16M/2.16M [00:00<00:00, 159MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import zipfile\n",
        "\n",
        "file_id = '10u97s_c2-ougNHQZMAP1jWvPZmWGIx1y'\n",
        "output_file = 'Test.zip'\n",
        "\n",
        "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "gdown.download(download_url, output_file, quiet=False)\n",
        "\n",
        "with zipfile.ZipFile(output_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"extracted_files/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JnrAVkJwMp_",
        "outputId": "1706632b-9c23-486d-e140-108c53a9faaa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10u97s_c2-ougNHQZMAP1jWvPZmWGIx1y\n",
            "To: /content/Test.zip\n",
            "100%|██████████| 542k/542k [00:00<00:00, 89.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = 'extracted_files/Train'\n",
        "test= 'extracted_files/Test'"
      ],
      "metadata": {
        "id": "tf5g1VwPHQLG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_images(base_dir, classes, image_size=IMAGE_SIZE):\n",
        "    \"\"\"Preprocess images into arrays and return data with labels.\"\"\"\n",
        "    data, labels = [], []\n",
        "    class_to_idx = {class_name: idx for idx, class_name in enumerate(classes)}\n",
        "    for class_name in classes:\n",
        "        class_dir = os.path.join(base_dir, class_name)\n",
        "        image_files = os.listdir(class_dir)\n",
        "        for image_file in image_files:\n",
        "            image_path = os.path.join(class_dir, image_file)\n",
        "            with Image.open(image_path) as img:\n",
        "                img_resized = img.resize(image_size).convert('RGB')\n",
        "                data.append(np.array(img_resized) / 255.0)\n",
        "                labels.append(class_to_idx[class_name])\n",
        "    return np.array(data), np.array(labels), class_to_idx"
      ],
      "metadata": {
        "id": "QMxHEVDpHXyv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(Z):\n",
        "    return np.maximum(0, Z)"
      ],
      "metadata": {
        "id": "p6TTd3IOHkqN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_derivative(Z):\n",
        "    return (Z > 0).astype(float)"
      ],
      "metadata": {
        "id": "6bzPNqAHHslS"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(Z):\n",
        "    expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
        "    return expZ / np.sum(expZ, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "UBWKnTjxHwGG"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(input_size, hidden_size, output_size):\n",
        "    np.random.seed(42)\n",
        "    W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "    b1 = np.zeros((1, hidden_size))\n",
        "    W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "    b2 = np.zeros((1, output_size))\n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "2jtnGNbPHzV5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(X, W1, b1, W2, b2, dropout_mask=None):\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = relu(Z1)\n",
        "    if dropout_mask is not None:\n",
        "        A1 *= dropout_mask\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = softmax(Z2)\n",
        "    cache = (Z1, A1, Z2, A2, dropout_mask)\n",
        "    return A2, cache"
      ],
      "metadata": {
        "id": "rtzLalXLH3fx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(Y, A2, W1, W2, l2_lambda):\n",
        "    m = Y.shape[0]\n",
        "    log_likelihood = -np.log(A2[range(m), Y])\n",
        "    loss = np.sum(log_likelihood) / m\n",
        "    l2_penalty = (l2_lambda / (2 * m)) * (np.sum(W1**2) + np.sum(W2**2))\n",
        "    return loss + l2_penalty"
      ],
      "metadata": {
        "id": "l7fhz32uH62Y"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation(X, Y, cache, W2, l2_lambda):\n",
        "    Z1, A1, Z2, A2, dropout_mask = cache\n",
        "    m = X.shape[0]\n",
        "    dZ2 = A2\n",
        "    dZ2[range(m), Y] -= 1\n",
        "    dZ2 /= m\n",
        "\n",
        "    dW2 = np.dot(A1.T, dZ2) + (l2_lambda / m) * W2\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "    dA1 = np.dot(dZ2, W2.T)\n",
        "    if dropout_mask is not None:\n",
        "        dA1 *= dropout_mask\n",
        "    dZ1 = dA1 * relu_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1) + (l2_lambda / m) * W1\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    return dW1, db1, dW2, db2"
      ],
      "metadata": {
        "id": "hRx3mPpCICuq"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_weights(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "KwRF9E5lIHqu"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, W1, b1, W2, b2):\n",
        "    A2, _ = forward_propagation(X, W1, b1, W2, b2)\n",
        "    return np.argmax(A2, axis=1)"
      ],
      "metadata": {
        "id": "gG5bP5yqIL8d"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_neural_network(train_data, train_labels, test_data, test_labels, input_size, hidden_size, output_size):\n",
        "    W1, b1, W2, b2 = initialize_weights(input_size, hidden_size, output_size)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        indices = np.random.permutation(train_data.shape[0])\n",
        "        train_data, train_labels = train_data[indices], train_labels[indices]\n",
        "\n",
        "        for i in range(0, train_data.shape[0], BATCH_SIZE):\n",
        "            X_batch = train_data[i:i + BATCH_SIZE]\n",
        "            Y_batch = train_labels[i:i + BATCH_SIZE]\n",
        "\n",
        "            dropout_mask = (np.random.rand(X_batch.shape[0], hidden_size) > DROPOUT_RATE).astype(float)\n",
        "\n",
        "            A2, cache = forward_propagation(X_batch, W1, b1, W2, b2, dropout_mask)\n",
        "            dW1, db1, dW2, db2 = backward_propagation(X_batch, Y_batch, cache, W2, L2_LAMBDA)\n",
        "\n",
        "            W1, b1, W2, b2 = update_weights(W1, b1, W2, b2, dW1, db1, dW2, db2, LEARNING_RATE)\n",
        "\n",
        "        A2, _ = forward_propagation(train_data, W1, b1, W2, b2)\n",
        "        loss = compute_loss(train_labels, A2, W1, W2, L2_LAMBDA)\n",
        "        print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {loss:.4f}\")\n",
        "\n",
        "    predictions = predict(test_data, W1, b1, W2, b2)\n",
        "    accuracy = np.mean(predictions == test_labels)*100\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "TX46vJF-IQHt"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_classes = os.listdir(train)\n",
        "test_classes = os.listdir(test)\n",
        "train_data, train_labels, class_mapping = preprocess_images(train, train_classes)\n",
        "test_data, test_labels, _ = preprocess_images(test, test_classes)"
      ],
      "metadata": {
        "id": "u93De9BxIiGl"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.reshape(train_data.shape[0], -1)\n",
        "test_data = test_data.reshape(test_data.shape[0], -1)\n",
        "\n",
        "input_size = train_data.shape[1]\n",
        "hidden_size = 64\n",
        "output_size = len(train_classes)\n",
        "train_neural_network(train_data, train_labels, test_data, test_labels, input_size, hidden_size, output_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq21lwOOIxda",
        "outputId": "8586a0cb-adbd-44bc-be6e-0dc6320cf5ce"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 1.4787\n",
            "Epoch 2/100, Loss: 1.1743\n",
            "Epoch 3/100, Loss: 0.9035\n",
            "Epoch 4/100, Loss: 0.6778\n",
            "Epoch 5/100, Loss: 0.5003\n",
            "Epoch 6/100, Loss: 0.3936\n",
            "Epoch 7/100, Loss: 0.3337\n",
            "Epoch 8/100, Loss: 0.2806\n",
            "Epoch 9/100, Loss: 0.2529\n",
            "Epoch 10/100, Loss: 0.2390\n",
            "Epoch 11/100, Loss: 0.2065\n",
            "Epoch 12/100, Loss: 0.2039\n",
            "Epoch 13/100, Loss: 0.1870\n",
            "Epoch 14/100, Loss: 0.1806\n",
            "Epoch 15/100, Loss: 0.1685\n",
            "Epoch 16/100, Loss: 0.1656\n",
            "Epoch 17/100, Loss: 0.1579\n",
            "Epoch 18/100, Loss: 0.1470\n",
            "Epoch 19/100, Loss: 0.1505\n",
            "Epoch 20/100, Loss: 0.1380\n",
            "Epoch 21/100, Loss: 0.1580\n",
            "Epoch 22/100, Loss: 0.1352\n",
            "Epoch 23/100, Loss: 0.1373\n",
            "Epoch 24/100, Loss: 0.1308\n",
            "Epoch 25/100, Loss: 0.1326\n",
            "Epoch 26/100, Loss: 0.1588\n",
            "Epoch 27/100, Loss: 0.1313\n",
            "Epoch 28/100, Loss: 0.1210\n",
            "Epoch 29/100, Loss: 0.1225\n",
            "Epoch 30/100, Loss: 0.1311\n",
            "Epoch 31/100, Loss: 0.1229\n",
            "Epoch 32/100, Loss: 0.1161\n",
            "Epoch 33/100, Loss: 0.1222\n",
            "Epoch 34/100, Loss: 0.1128\n",
            "Epoch 35/100, Loss: 0.1111\n",
            "Epoch 36/100, Loss: 0.1102\n",
            "Epoch 37/100, Loss: 0.1105\n",
            "Epoch 38/100, Loss: 0.1092\n",
            "Epoch 39/100, Loss: 0.1102\n",
            "Epoch 40/100, Loss: 0.1118\n",
            "Epoch 41/100, Loss: 0.1163\n",
            "Epoch 42/100, Loss: 0.1082\n",
            "Epoch 43/100, Loss: 0.1088\n",
            "Epoch 44/100, Loss: 0.1082\n",
            "Epoch 45/100, Loss: 0.1131\n",
            "Epoch 46/100, Loss: 0.1034\n",
            "Epoch 47/100, Loss: 0.1108\n",
            "Epoch 48/100, Loss: 0.1120\n",
            "Epoch 49/100, Loss: 0.1079\n",
            "Epoch 50/100, Loss: 0.1048\n",
            "Epoch 51/100, Loss: 0.1102\n",
            "Epoch 52/100, Loss: 0.1077\n",
            "Epoch 53/100, Loss: 0.1008\n",
            "Epoch 54/100, Loss: 0.0986\n",
            "Epoch 55/100, Loss: 0.1053\n",
            "Epoch 56/100, Loss: 0.1060\n",
            "Epoch 57/100, Loss: 0.0996\n",
            "Epoch 58/100, Loss: 0.1007\n",
            "Epoch 59/100, Loss: 0.0962\n",
            "Epoch 60/100, Loss: 0.0989\n",
            "Epoch 61/100, Loss: 0.1005\n",
            "Epoch 62/100, Loss: 0.0995\n",
            "Epoch 63/100, Loss: 0.1005\n",
            "Epoch 64/100, Loss: 0.0943\n",
            "Epoch 65/100, Loss: 0.1052\n",
            "Epoch 66/100, Loss: 0.0936\n",
            "Epoch 67/100, Loss: 0.0941\n",
            "Epoch 68/100, Loss: 0.0964\n",
            "Epoch 69/100, Loss: 0.0969\n",
            "Epoch 70/100, Loss: 0.0942\n",
            "Epoch 71/100, Loss: 0.0903\n",
            "Epoch 72/100, Loss: 0.0927\n",
            "Epoch 73/100, Loss: 0.0926\n",
            "Epoch 74/100, Loss: 0.0895\n",
            "Epoch 75/100, Loss: 0.0925\n",
            "Epoch 76/100, Loss: 0.0931\n",
            "Epoch 77/100, Loss: 0.0902\n",
            "Epoch 78/100, Loss: 0.0908\n",
            "Epoch 79/100, Loss: 0.0918\n",
            "Epoch 80/100, Loss: 0.0898\n",
            "Epoch 81/100, Loss: 0.0981\n",
            "Epoch 82/100, Loss: 0.1018\n",
            "Epoch 83/100, Loss: 0.0942\n",
            "Epoch 84/100, Loss: 0.0846\n",
            "Epoch 85/100, Loss: 0.0968\n",
            "Epoch 86/100, Loss: 0.0884\n",
            "Epoch 87/100, Loss: 0.0916\n",
            "Epoch 88/100, Loss: 0.0865\n",
            "Epoch 89/100, Loss: 0.0875\n",
            "Epoch 90/100, Loss: 0.0892\n",
            "Epoch 91/100, Loss: 0.0856\n",
            "Epoch 92/100, Loss: 0.0867\n",
            "Epoch 93/100, Loss: 0.0851\n",
            "Epoch 94/100, Loss: 0.0820\n",
            "Epoch 95/100, Loss: 0.0850\n",
            "Epoch 96/100, Loss: 0.0916\n",
            "Epoch 97/100, Loss: 0.0796\n",
            "Epoch 98/100, Loss: 0.0937\n",
            "Epoch 99/100, Loss: 0.0795\n",
            "Epoch 100/100, Loss: 0.0803\n",
            "Test Accuracy: 95.8000\n"
          ]
        }
      ]
    }
  ]
}